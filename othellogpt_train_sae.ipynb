{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n",
    "\n",
    "dataset_path = 'taufeeque/othellogpt'\n",
    "model_name = 'othello-gpt'\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "for exp_factor in [1, 2, 4, 8]:\n",
    "    config = LanguageModelSAERunnerConfig(\n",
    "        model_name=model_name,\n",
    "        hook_point=\"blocks.6.hook_resid_pre\",\n",
    "        hook_point_layer=6,\n",
    "        dataset_path=dataset_path,\n",
    "        context_size=59,\n",
    "        d_in=512,\n",
    "        n_batches_in_buffer=32,\n",
    "        total_training_tokens=100*(1e6), # prev: 10*(1e6)\n",
    "        store_batch_size=32,\n",
    "        device=device,\n",
    "        seed=42,\n",
    "        dtype=torch.float32,\n",
    "        b_dec_init_method=\"geometric_median\", # todo: geometric_median\n",
    "        expansion_factor=exp_factor, # todo: adjust\n",
    "        l1_coefficient=0.0002, # prev: 0.001, 0.0001\n",
    "        lr=0.00003, # prev: 0.0003\n",
    "        lr_scheduler_name=\"constantwithwarmup\",\n",
    "        lr_warm_up_steps=5000,\n",
    "        train_batch_size=4096,\n",
    "        use_ghost_grads=True,\n",
    "        feature_sampling_window=500,\n",
    "        dead_feature_window=1e6,\n",
    "        log_to_wandb=True,\n",
    "        wandb_project=\"othello_gpt_sae\",\n",
    "        wandb_log_frequency=30,\n",
    "        n_checkpoints=0,\n",
    "        checkpoint_path=\"checkpoints\",\n",
    "        start_pos_offset=1, # exclude first seq position\n",
    "    )\n",
    "\n",
    "    sparse_autoencoder = language_model_sae_runner(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats_sae_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
